# ðŸŽ¯ MLflow UI EXPLAINED - WHAT YOU SEE & WHY

## ðŸ¤” YOUR QUESTION: "What is MLflow doing and what is that UI showing?"

**Simple Answer:** MLflow is like GitHub for ML models. It tracks every experiment, stores model versions, and helps you compare which model is best.

---

## ðŸ“Š WHAT IS MLflow?

### **Think of it like this:**

**GitHub** â†’ Versions your code  
**MLflow** â†’ Versions your models

**Google Drive** â†’ Stores files  
**MLflow** â†’ Stores models + their metrics

**Notebook** â†’ Writes what you did  
**MLflow** â†’ Records every experiment automatically

---

## ðŸ–¥ï¸ THE MLflow UI - WHAT YOU SEE

### **Starting MLflow UI:**
```powershell
mlflow ui
# Opens: http://localhost:5000
```

### **Main Sections of the UI:**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  MLflow - Experiment Tracking               â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                             â”‚
â”‚  ðŸ“ Experiments  (left sidebar)             â”‚
â”‚  â”œâ”€â”€ Default                                â”‚
â”‚  â””â”€â”€ Air Quality Experiments               â”‚
â”‚                                             â”‚
â”‚  ðŸ“Š Runs  (main area)                       â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”‚
â”‚  â”‚ Run Name â”‚ Metrics  â”‚ Params   â”‚        â”‚
â”‚  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤        â”‚
â”‚  â”‚ run_001  â”‚ acc=0.95 â”‚ n_est=100â”‚        â”‚
â”‚  â”‚ run_002  â”‚ acc=0.98 â”‚ n_est=200â”‚        â”‚
â”‚  â”‚ run_003  â”‚ acc=0.92 â”‚ n_est=50 â”‚        â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â”‚
â”‚                                             â”‚
â”‚  ðŸ“ˆ Compare  (when multiple selected)       â”‚
â”‚  [Shows graphs comparing metrics]           â”‚
â”‚                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ðŸ“‹ UI SECTIONS EXPLAINED

### **1. Experiments Tab**

**What it shows:**
- List of all your experiment groups
- Like folders for organizing different projects

**Example:**
```
ðŸ“ Experiments
  â”œâ”€â”€ Air Quality Classification
  â”œâ”€â”€ PM2.5 Prediction
  â””â”€â”€ City Clustering
```

**What to say in viva:**
"This shows my organized experiments. Each experiment contains multiple runs with different hyperparameters."

---

### **2. Runs Table**

**What it shows:**
Each row = One time you trained a model

**Columns:**
| Column | Shows | Example |
|--------|-------|---------|
| **Start Time** | When trained | 2025-12-17 10:30 |
| **Duration** | How long | 2.5 minutes |
| **Metrics** | Performance | accuracy: 0.95 |
| **Parameters** | Settings | n_estimators: 100 |
| **Tags** | Labels | version: v1.0 |

**What to say in viva:**
"Each run represents one training session. I can see all metrics and parameters to compare which configuration performed best."

---

### **3. Run Details Page**

**Click on any run to see:**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Run: aqi_classifier_20251217       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                     â”‚
â”‚  ðŸ“Š METRICS                         â”‚
â”‚    accuracy:    1.0000              â”‚
â”‚    f1_score:    1.0000              â”‚
â”‚    precision:   1.0000              â”‚
â”‚    recall:      1.0000              â”‚
â”‚                                     â”‚
â”‚  âš™ï¸ PARAMETERS                      â”‚
â”‚    n_estimators:     100            â”‚
â”‚    max_depth:        10             â”‚
â”‚    min_samples_split: 5             â”‚
â”‚    random_state:     42             â”‚
â”‚                                     â”‚
â”‚  ðŸ·ï¸ TAGS                            â”‚
â”‚    model_type:       classification â”‚
â”‚    algorithm:        random_forest  â”‚
â”‚    created_by:       Bingbang       â”‚
â”‚                                     â”‚
â”‚  ðŸ“¦ ARTIFACTS                       â”‚
â”‚    model/                           â”‚
â”‚    â””â”€â”€ model.pkl                    â”‚
â”‚    â””â”€â”€ requirements.txt             â”‚
â”‚    â””â”€â”€ conda.yaml                   â”‚
â”‚                                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**What to say in viva:**
"Clicking on a run shows complete details: all metrics, hyperparameters, and the saved model artifact."

---

### **4. Compare Runs**

**Select multiple runs and click "Compare":**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Comparing 3 runs                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                     â”‚
â”‚  ðŸ“ˆ Accuracy Comparison             â”‚
â”‚     Run 1: â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ 0.95        â”‚
â”‚     Run 2: â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ 0.98       â”‚
â”‚     Run 3: â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ 0.92          â”‚
â”‚                                     â”‚
â”‚  âš™ï¸ Parameter Differences           â”‚
â”‚     n_estimators: 100 | 200 | 50   â”‚
â”‚     max_depth:     10 |  15 |  5   â”‚
â”‚                                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**What to say in viva:**
"The compare feature helps me identify which hyperparameters led to best performance across multiple experiments."

---

### **5. Models Registry**

**Navigate to "Models" tab:**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Registered Models                  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                     â”‚
â”‚  ðŸ“¦ aqi_classifier                  â”‚
â”‚    Version 1: Production            â”‚
â”‚    Version 2: Staging               â”‚
â”‚    Version 3: Archived              â”‚
â”‚                                     â”‚
â”‚  ðŸ“¦ pm25_regressor                  â”‚
â”‚    Version 1: Production            â”‚
â”‚                                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**What to say in viva:**
"Model registry manages model lifecycle. Models move through stages: Development â†’ Staging â†’ Production â†’ Archived."

---

## ðŸŽ¯ WHAT MLFLOW DOES (Simple Explanation)

### **1. Tracks Experiments**
```python
# Every time you train:
mlflow.log_params({"n_estimators": 100})
mlflow.log_metrics({"accuracy": 0.95})
```
â†’ MLflow records it automatically

### **2. Stores Models**
```python
mlflow.sklearn.log_model(model, "model")
```
â†’ Saves the actual model file

### **3. Compares Results**
â†’ Which hyperparameters work best?
â†’ Which model version to deploy?

### **4. Manages Versions**
```
v1.0 â†’ Trained on Jan data
v2.0 â†’ Trained on Feb data (better!)
v3.0 â†’ Trained on Mar data (worse, rollback!)
```

### **5. Enables Rollback**
â†’ New model bad? â†’ Load previous version

---

## ðŸŽ¬ FOR YOUR VIVA DEMO

### **Step 1: Register Models**
```powershell
python src/model_registry.py
```
**Output:**
```
==============================================================
REGISTERING MODELS WITH MLFLOW
==============================================================

âœ… AQI Classifier registered
âœ… PM2.5 Regressor registered
```

### **Step 2: Start MLflow UI**
```powershell
mlflow ui
```
**Opens:** http://localhost:5000

### **Step 3: Show in Browser**

**Point to screen and say:**

"Here you can see:
1. **Experiments tab** - All my training runs
2. **Each run shows** - Metrics like accuracy (100%) and RÂ² (0.924)
3. **Parameters** - Hyperparameters used (n_estimators, max_depth)
4. **Artifacts** - The saved model files
5. **Models tab** - Registered models with version control"

### **Step 4: Show Comparison**
- Select 2 runs
- Click "Compare"
- Show graphs

**Say:** "I can compare different runs to see which hyperparameters performed best."

---

## ðŸ“Š WHAT EACH METRIC MEANS

When you see these in MLflow:

| Metric | Meaning | Good Value |
|--------|---------|------------|
| **accuracy** | % correct predictions | >0.90 (90%+) |
| **f1_score** | Balance of precision & recall | >0.90 |
| **r2_score** | How well model fits | >0.80 |
| **rmse** | Average prediction error | Lower is better |
| **mae** | Absolute error | Lower is better |

---

## ðŸŽ“ VIVA QUESTIONS ABOUT MLFLOW

### Q: "Why use MLflow?"
**A:** "MLflow provides model versioning, experiment tracking, and lifecycle management. Without it, I'd have to manually track which model version is in production and what metrics it achieved."

### Q: "What's in the artifacts?"
**A:** "Artifacts include the trained model (.pkl file), requirements.txt for dependencies, and metadata about the training environment."

### Q: "How do you deploy from MLflow?"
**A:** "I can load any model version directly:
```python
model = mlflow.sklearn.load_model('models:/aqi_classifier/Production')
```
This loads the model marked as 'Production' stage."

### Q: "What if a new model is worse?"
**A:** "MLflow allows instant rollback. I can transition the previous version back to 'Production' stage without retraining."

---

## ðŸ”„ COMPLETE MLFLOW WORKFLOW

```
1. TRAIN MODEL
   â†“
   mlflow.start_run()
   
2. LOG EVERYTHING
   â†“
   mlflow.log_params()
   mlflow.log_metrics()
   mlflow.log_model()
   
3. VIEW IN UI
   â†“
   mlflow ui
   
4. COMPARE RUNS
   â†“
   Select multiple â†’ Compare
   
5. REGISTER BEST MODEL
   â†“
   Model Registry â†’ Version 1
   
6. PROMOTE TO PRODUCTION
   â†“
   Stage: None â†’ Staging â†’ Production
   
7. DEPLOY
   â†“
   Load from registry
   
8. MONITOR
   â†“
   If performance drops â†’ Rollback or Retrain
```

---

## âœ… QUICK CHECKLIST FOR VIVA

**Be ready to:**
- [ ] Start MLflow UI (`mlflow ui`)
- [ ] Show experiments list
- [ ] Point to a run and explain metrics
- [ ] Show comparison between runs
- [ ] Explain model registry stages
- [ ] Explain why it's useful

**One sentence summary:**
"MLflow tracks experiments, stores models with their metrics, and manages model versions for production deployment."

---

## ðŸ’¡ PRO TIP

**If MLflow UI is empty:**
```powershell
# Register models first
python src/model_registry.py

# Then start UI
mlflow ui
```

**If they ask "Can you show it?":**
1. Open http://localhost:5000
2. Click on "Experiments"
3. Click on any run
4. Point to metrics and say: "Here's accuracy 100% with these hyperparameters"
5. Click "Models" tab
6. Show registered models

**EASY!** âœ¨

---

## ðŸŽ¯ REMEMBER

**MLflow UI shows 5 things:**
1. **Experiments** - Groups of training runs
2. **Metrics** - How good each model is
3. **Parameters** - Settings used for each run
4. **Artifacts** - Saved model files
5. **Registry** - Production model versions

**Say this:** "MLflow is my model registry that tracks all experiments, versions models, and helps me manage which model is deployed in production."

**PERFECT ANSWER!** ðŸŽ‰
